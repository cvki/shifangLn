[2022-11-14 15:37:18] [INFO] Command executed: /d2/code/gitRes/Expts/GeoTransformer-main/experiments/geotransformer.modelnet.rpmnet.stage4.gse.k3.max.oacl.stage2.sinkhorn/trainval.py
[2022-11-14 15:37:18] [INFO] Configs:
{
    "seed": 7351,
    "working_dir": "/d2/code/gitRes/Expts/GeoTransformer-main/experiments/geotransformer.modelnet.rpmnet.stage4.gse.k3.max.oacl.stage2.sinkhorn",
    "root_dir": "/d2/code/gitRes/Expts/GeoTransformer-main",
    "exp_name": "geotransformer.modelnet.rpmnet.stage4.gse.k3.max.oacl.stage2.sinkhorn",
    "output_dir": "/d2/code/gitRes/Expts/GeoTransformer-main/output/geotransformer.modelnet.rpmnet.stage4.gse.k3.max.oacl.stage2.sinkhorn",
    "snapshot_dir": "/d2/code/gitRes/Expts/GeoTransformer-main/output/geotransformer.modelnet.rpmnet.stage4.gse.k3.max.oacl.stage2.sinkhorn/snapshots",
    "log_dir": "/d2/code/gitRes/Expts/GeoTransformer-main/output/geotransformer.modelnet.rpmnet.stage4.gse.k3.max.oacl.stage2.sinkhorn/logs",
    "event_dir": "/d2/code/gitRes/Expts/GeoTransformer-main/output/geotransformer.modelnet.rpmnet.stage4.gse.k3.max.oacl.stage2.sinkhorn/events",
    "data": {
        "dataset_root": "/d2/code/gitRes/Expts/GeoTransformer-main/data/ModelNet",
        "num_points": 717,
        "voxel_size": null,
        "rotation_magnitude": 180.0,
        "translation_magnitude": 0.5,
        "keep_ratio": 0.7,
        "crop_method": "plane",
        "asymmetric": true,
        "twice_sample": true,
        "twice_transform": false
    },
    "train": {
        "batch_size": 1,
        "num_workers": 8,
        "noise_magnitude": 0.05,
        "class_indices": "all"
    },
    "test": {
        "batch_size": 1,
        "num_workers": 8,
        "noise_magnitude": 0.05,
        "class_indices": "all"
    },
    "eval": {
        "acceptance_overlap": 0.0,
        "acceptance_radius": 0.1,
        "inlier_ratio_threshold": 0.05,
        "rre_threshold": 1.0,
        "rte_threshold": 0.1
    },
    "ransac": {
        "distance_threshold": 0.05,
        "num_points": 3,
        "num_iterations": 1000
    },
    "optim": {
        "lr": 0.0001,
        "weight_decay": 1e-06,
        "warmup_steps": 10000,
        "eta_init": 0.1,
        "eta_min": 0.1,
        "max_iteration": 400000,
        "snapshot_steps": 10000,
        "grad_acc_steps": 1
    },
    "backbone": {
        "num_stages": 3,
        "init_voxel_size": 0.05,
        "kernel_size": 15,
        "base_radius": 2.5,
        "base_sigma": 2.0,
        "init_radius": 0.125,
        "init_sigma": 0.1,
        "group_norm": 32,
        "input_dim": 1,
        "init_dim": 64,
        "output_dim": 256
    },
    "model": {
        "ground_truth_matching_radius": 0.05,
        "num_points_in_patch": 128,
        "num_sinkhorn_iterations": 100
    },
    "coarse_matching": {
        "num_targets": 128,
        "overlap_threshold": 0.1,
        "num_correspondences": 128,
        "dual_normalization": true
    },
    "geotransformer": {
        "input_dim": 512,
        "hidden_dim": 256,
        "output_dim": 256,
        "num_heads": 4,
        "blocks": [
            "self",
            "cross",
            "self",
            "cross",
            "self",
            "cross"
        ],
        "sigma_d": 0.2,
        "sigma_a": 15,
        "angle_k": 3,
        "reduction_a": "max"
    },
    "fine_matching": {
        "topk": 3,
        "acceptance_radius": 0.1,
        "mutual": true,
        "confidence_threshold": 0.05,
        "use_dustbin": false,
        "use_global_score": false,
        "correspondence_threshold": 3,
        "correspondence_limit": null,
        "num_refinement_steps": 5
    },
    "coarse_loss": {
        "positive_margin": 0.1,
        "negative_margin": 1.4,
        "positive_optimal": 0.1,
        "negative_optimal": 1.4,
        "log_scale": 24,
        "positive_overlap": 0.1
    },
    "fine_loss": {
        "positive_radius": 0.05
    },
    "loss": {
        "weight_coarse_loss": 1.0,
        "weight_fine_loss": 1.0
    }
}
[2022-11-14 15:37:18] [INFO] Tensorboard is enabled. Write events to /d2/code/gitRes/Expts/GeoTransformer-main/output/geotransformer.modelnet.rpmnet.stage4.gse.k3.max.oacl.stage2.sinkhorn/events.
[2022-11-14 15:37:18] [INFO] Using Single-GPU mode.
[2022-11-14 15:37:18] [INFO] Data loader created: 0.318s collapsed.
[2022-11-14 15:37:18] [INFO] Calibrate neighbors: [23 35 40].
[2022-11-14 15:37:21] [INFO] Model description:
GeoTransformer(
  (backbone): KPConvFPN(
    (encoder1_1): ConvBlock(
      (KPConv): KPConv(kernel_size: 15, in_channels: 1, out_channels: 64, radius: 0.125, sigma: 0.1, bias: True)
      (norm): GroupNorm(
        (norm): GroupNorm(32, 64, eps=1e-05, affine=True)
      )
      (leaky_relu): LeakyReLU(negative_slope=0.1)
    )
    (encoder1_2): ResidualBlock(
      (unary1): UnaryBlock(
        (mlp): Linear(in_features=64, out_features=32, bias=True)
        (norm): GroupNorm(
          (norm): GroupNorm(32, 32, eps=1e-05, affine=True)
        )
        (leaky_relu): LeakyReLU(negative_slope=0.1)
      )
      (KPConv): KPConv(kernel_size: 15, in_channels: 32, out_channels: 32, radius: 0.125, sigma: 0.1, bias: True)
      (norm_conv): GroupNorm(
        (norm): GroupNorm(32, 32, eps=1e-05, affine=True)
      )
      (unary2): UnaryBlock(
        (mlp): Linear(in_features=32, out_features=128, bias=True)
        (norm): GroupNorm(
          (norm): GroupNorm(32, 128, eps=1e-05, affine=True)
        )
      )
      (unary_shortcut): UnaryBlock(
        (mlp): Linear(in_features=64, out_features=128, bias=True)
        (norm): GroupNorm(
          (norm): GroupNorm(32, 128, eps=1e-05, affine=True)
        )
      )
      (leaky_relu): LeakyReLU(negative_slope=0.1)
    )
    (encoder2_1): ResidualBlock(
      (unary1): UnaryBlock(
        (mlp): Linear(in_features=128, out_features=32, bias=True)
        (norm): GroupNorm(
          (norm): GroupNorm(32, 32, eps=1e-05, affine=True)
        )
        (leaky_relu): LeakyReLU(negative_slope=0.1)
      )
      (KPConv): KPConv(kernel_size: 15, in_channels: 32, out_channels: 32, radius: 0.125, sigma: 0.1, bias: True)
      (norm_conv): GroupNorm(
        (norm): GroupNorm(32, 32, eps=1e-05, affine=True)
      )
      (unary2): UnaryBlock(
        (mlp): Linear(in_features=32, out_features=128, bias=True)
        (norm): GroupNorm(
          (norm): GroupNorm(32, 128, eps=1e-05, affine=True)
        )
      )
      (unary_shortcut): Identity()
      (leaky_relu): LeakyReLU(negative_slope=0.1)
    )
    (encoder2_2): ResidualBlock(
      (unary1): UnaryBlock(
        (mlp): Linear(in_features=128, out_features=64, bias=True)
        (norm): GroupNorm(
          (norm): GroupNorm(32, 64, eps=1e-05, affine=True)
        )
        (leaky_relu): LeakyReLU(negative_slope=0.1)
      )
      (KPConv): KPConv(kernel_size: 15, in_channels: 64, out_channels: 64, radius: 0.25, sigma: 0.2, bias: True)
      (norm_conv): GroupNorm(
        (norm): GroupNorm(32, 64, eps=1e-05, affine=True)
      )
      (unary2): UnaryBlock(
        (mlp): Linear(in_features=64, out_features=256, bias=True)
        (norm): GroupNorm(
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (unary_shortcut): UnaryBlock(
        (mlp): Linear(in_features=128, out_features=256, bias=True)
        (norm): GroupNorm(
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (leaky_relu): LeakyReLU(negative_slope=0.1)
    )
    (encoder2_3): ResidualBlock(
      (unary1): UnaryBlock(
        (mlp): Linear(in_features=256, out_features=64, bias=True)
        (norm): GroupNorm(
          (norm): GroupNorm(32, 64, eps=1e-05, affine=True)
        )
        (leaky_relu): LeakyReLU(negative_slope=0.1)
      )
      (KPConv): KPConv(kernel_size: 15, in_channels: 64, out_channels: 64, radius: 0.25, sigma: 0.2, bias: True)
      (norm_conv): GroupNorm(
        (norm): GroupNorm(32, 64, eps=1e-05, affine=True)
      )
      (unary2): UnaryBlock(
        (mlp): Linear(in_features=64, out_features=256, bias=True)
        (norm): GroupNorm(
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (unary_shortcut): Identity()
      (leaky_relu): LeakyReLU(negative_slope=0.1)
    )
    (encoder3_1): ResidualBlock(
      (unary1): UnaryBlock(
        (mlp): Linear(in_features=256, out_features=64, bias=True)
        (norm): GroupNorm(
          (norm): GroupNorm(32, 64, eps=1e-05, affine=True)
        )
        (leaky_relu): LeakyReLU(negative_slope=0.1)
      )
      (KPConv): KPConv(kernel_size: 15, in_channels: 64, out_channels: 64, radius: 0.25, sigma: 0.2, bias: True)
      (norm_conv): GroupNorm(
        (norm): GroupNorm(32, 64, eps=1e-05, affine=True)
      )
      (unary2): UnaryBlock(
        (mlp): Linear(in_features=64, out_features=256, bias=True)
        (norm): GroupNorm(
          (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (unary_shortcut): Identity()
      (leaky_relu): LeakyReLU(negative_slope=0.1)
    )
    (encoder3_2): ResidualBlock(
      (unary1): UnaryBlock(
        (mlp): Linear(in_features=256, out_features=128, bias=True)
        (norm): GroupNorm(
          (norm): GroupNorm(32, 128, eps=1e-05, affine=True)
        )
        (leaky_relu): LeakyReLU(negative_slope=0.1)
      )
      (KPConv): KPConv(kernel_size: 15, in_channels: 128, out_channels: 128, radius: 0.5, sigma: 0.4, bias: True)
      (norm_conv): GroupNorm(
        (norm): GroupNorm(32, 128, eps=1e-05, affine=True)
      )
      (unary2): UnaryBlock(
        (mlp): Linear(in_features=128, out_features=512, bias=True)
        (norm): GroupNorm(
          (norm): GroupNorm(32, 512, eps=1e-05, affine=True)
        )
      )
      (unary_shortcut): UnaryBlock(
        (mlp): Linear(in_features=256, out_features=512, bias=True)
        (norm): GroupNorm(
          (norm): GroupNorm(32, 512, eps=1e-05, affine=True)
        )
      )
      (leaky_relu): LeakyReLU(negative_slope=0.1)
    )
    (encoder3_3): ResidualBlock(
      (unary1): UnaryBlock(
        (mlp): Linear(in_features=512, out_features=128, bias=True)
        (norm): GroupNorm(
          (norm): GroupNorm(32, 128, eps=1e-05, affine=True)
        )
        (leaky_relu): LeakyReLU(negative_slope=0.1)
      )
      (KPConv): KPConv(kernel_size: 15, in_channels: 128, out_channels: 128, radius: 0.5, sigma: 0.4, bias: True)
      (norm_conv): GroupNorm(
        (norm): GroupNorm(32, 128, eps=1e-05, affine=True)
      )
      (unary2): UnaryBlock(
        (mlp): Linear(in_features=128, out_features=512, bias=True)
        (norm): GroupNorm(
          (norm): GroupNorm(32, 512, eps=1e-05, affine=True)
        )
      )
      (unary_shortcut): Identity()
      (leaky_relu): LeakyReLU(negative_slope=0.1)
    )
    (decoder2): UnaryBlock(
      (mlp): Linear(in_features=768, out_features=256, bias=True)
      (norm): GroupNorm(
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (leaky_relu): LeakyReLU(negative_slope=0.1)
    )
    (decoder1): LastUnaryBlock(
      (mlp): Linear(in_features=384, out_features=256, bias=True)
    )
  )
  (transformer): GeometricTransformer(
    (embedding): GeometricStructureEmbedding(
      (embedding): SinusoidalPositionalEmbedding()
      (proj_d): Linear(in_features=256, out_features=256, bias=True)
      (proj_a): Linear(in_features=256, out_features=256, bias=True)
    )
    (in_proj): Linear(in_features=512, out_features=256, bias=True)
    (transformer): RPEConditionalTransformer(
      (layers): ModuleList(
        (0): RPETransformerLayer(
          (attention): RPEAttentionLayer(
            (attention): RPEMultiHeadAttention(
              (proj_q): Linear(in_features=256, out_features=256, bias=True)
              (proj_k): Linear(in_features=256, out_features=256, bias=True)
              (proj_v): Linear(in_features=256, out_features=256, bias=True)
              (proj_p): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Identity()
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Identity()
            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (output): AttentionOutput(
            (expand): Linear(in_features=256, out_features=512, bias=True)
            (activation): ReLU()
            (squeeze): Linear(in_features=512, out_features=256, bias=True)
            (dropout): Identity()
            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): TransformerLayer(
          (attention): AttentionLayer(
            (attention): MultiHeadAttention(
              (proj_q): Linear(in_features=256, out_features=256, bias=True)
              (proj_k): Linear(in_features=256, out_features=256, bias=True)
              (proj_v): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Identity()
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Identity()
            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (output): AttentionOutput(
            (expand): Linear(in_features=256, out_features=512, bias=True)
            (activation): ReLU()
            (squeeze): Linear(in_features=512, out_features=256, bias=True)
            (dropout): Identity()
            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): RPETransformerLayer(
          (attention): RPEAttentionLayer(
            (attention): RPEMultiHeadAttention(
              (proj_q): Linear(in_features=256, out_features=256, bias=True)
              (proj_k): Linear(in_features=256, out_features=256, bias=True)
              (proj_v): Linear(in_features=256, out_features=256, bias=True)
              (proj_p): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Identity()
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Identity()
            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (output): AttentionOutput(
            (expand): Linear(in_features=256, out_features=512, bias=True)
            (activation): ReLU()
            (squeeze): Linear(in_features=512, out_features=256, bias=True)
            (dropout): Identity()
            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): TransformerLayer(
          (attention): AttentionLayer(
            (attention): MultiHeadAttention(
              (proj_q): Linear(in_features=256, out_features=256, bias=True)
              (proj_k): Linear(in_features=256, out_features=256, bias=True)
              (proj_v): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Identity()
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Identity()
            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (output): AttentionOutput(
            (expand): Linear(in_features=256, out_features=512, bias=True)
            (activation): ReLU()
            (squeeze): Linear(in_features=512, out_features=256, bias=True)
            (dropout): Identity()
            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (4): RPETransformerLayer(
          (attention): RPEAttentionLayer(
            (attention): RPEMultiHeadAttention(
              (proj_q): Linear(in_features=256, out_features=256, bias=True)
              (proj_k): Linear(in_features=256, out_features=256, bias=True)
              (proj_v): Linear(in_features=256, out_features=256, bias=True)
              (proj_p): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Identity()
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Identity()
            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (output): AttentionOutput(
            (expand): Linear(in_features=256, out_features=512, bias=True)
            (activation): ReLU()
            (squeeze): Linear(in_features=512, out_features=256, bias=True)
            (dropout): Identity()
            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (5): TransformerLayer(
          (attention): AttentionLayer(
            (attention): MultiHeadAttention(
              (proj_q): Linear(in_features=256, out_features=256, bias=True)
              (proj_k): Linear(in_features=256, out_features=256, bias=True)
              (proj_v): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Identity()
            )
            (linear): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Identity()
            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (output): AttentionOutput(
            (expand): Linear(in_features=256, out_features=512, bias=True)
            (activation): ReLU()
            (squeeze): Linear(in_features=512, out_features=256, bias=True)
            (dropout): Identity()
            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (out_proj): Linear(in_features=256, out_features=256, bias=True)
  )
  (coarse_target): SuperPointTargetGenerator()
  (coarse_matching): SuperPointMatching()
  (fine_matching): LocalGlobalRegistration(
    (procrustes): WeightedProcrustes()
  )
  (optimal_transport): LearnableLogOptimalTransport(num_iterations=100)
)